#!/usr/bin/env python
""" getpage2 - A Program to fetch a webpage and save it in a single HTML.

    Image will be stored as data-uri based64 string.
"""
"""
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""
import os
import argparse
import urllib2
from BeautifulSoup import BeautifulSoup, Tag
import urlparse
import logging
from readability.readability import Document


class GetPage():

    """ Get a webpage, and save it in a single file  the URL of a HTML Page.
        Returns a MIME-HTML representation of that page in a single File.
        There are a number of document formats
        consisting of a root resource and a number of distinct
        subsidiary resources referenced by URIs within that root resource.

        There is an obvious need to be able to save such multi-resource
        documents in a single file.
    """
    def __init__(self, options):
        self._title = ''
        self.options = options
        self._urls = {}
        self. _msg = None

    def add_html(self, url):
        """ Takes the URL of a HTML Page.
        """
        self._base_url = url
        response = urllib2.urlopen(url)
        html = response.read()
        doc = Document(html)
        readable_html = doc.summary()
        readable_title = doc.title()

        soup = BeautifulSoup(readable_html)

        # insert head.
        self._insert_head(soup)

        if self._title == '':
            self._title = readable_title.replace(u'/', u'_').replace(
                u'\\', u'_').replace(u'-', u'_').replace(u'"', u'').strip()
            title = Tag(soup, "title")
            title.setString(self._title)
            soup.head.insert(0, title)

        # insert origin.
        self._insert_origin_div(soup, url, readable_title)

        images = soup.findAll('img')
        for img in images:
            url = urlparse.urljoin(self._base_url, img['src'])
            if not url in self._urls:
                self._urls[url] = url
                img['src'] = self.get_image_as_base64(url)

        self._msg = soup.prettify()

    def _insert_head(self, soup):
        soup.html.insert(0, Tag(soup, "head"))
        meta = Tag(soup, "meta")
        meta.attrs.append(('http-equiv', "Content-Type"))
        meta.attrs.append(('content', "text/html; charset=utf-8;"))
        soup.head.insert(0, meta)
        meta = Tag(soup, "meta")
        meta.attrs.append(('charset', 'UTF-8'))
        soup.head.insert(0, meta)

    def _insert_origin_div(self, soup, url, title):
        div = Tag(soup, "div")
        span = Tag(soup, "span")
        link = self._make_anchor(soup, url, title)
        span.attrs.append(('class', 'origin'))
        span.insert(0, link)
        div.insert(0, span)
        soup.body.insert(0, div)

    def _make_anchor(self, soup, href, title):
        link = Tag(soup, "a")
        link.attrs.append(('href', href))
        link.attrs.append(('title', title))
        link.setString(title)
        return link

    def get_image_as_base64(self, url):
        """ Takes the URL of a HTML Page. Returns a base64 string.
        """
        try:
            response = urllib2.urlopen(url)
            mime_type = response.info().gettype()
            img = response.read()

            base64str = img.encode("base64").replace('\n', '')
            return 'data:{0};base64,{1}'.format(mime_type, base64str)
        except urllib2.HTTPError, err:
            if err.code == 404:
                logging.error("Page not found! {0}".format(url))
            elif err.code == 403:
                logging.error("Access denied!")
            else:
                logging.error("Something happened! Error code {0}".format(
                    err.code))
        except urllib2.URLError, err:
            logging.error("Some other error happened: {0}".format(err.reason))

    def retrieve(self, url):
        """ Takes the URL of a HTML Page. Returns a MIME encapsulated form of
            the HTML Code.

            If the HTML page itself links to images, scripts or stylesheet it
            calls the matching encapsulation Methods for them recursivly

        """

        self.add_html(url)

        dirs = [os.getcwd()]
        if self.options.output_directory:
            try:
                os.makedirs(self.options.output_directory)
            except OSError, ex:
                print("Skip to make dirs.")
            dirs.append(self.options.output_directory)
        if self.options.filename:
            dirs.append(self.options.filename)
        else:
            dirs.append("{0}.html".format(self._title.encode('utf-8')))
        filename = os.path.join(*dirs)
        f = open(filename, 'w')
        f.write(self._msg)
        f.close

if __name__ == '__main__':
    print("getpage2 0.0.1, a non-interactive webpage downloader")
    """ Takes the URL of a HTML Page.

        If the HTML page itself links to images, scripts or stylesheet it calls
        the matching encapsulation Methods for them recursivly
    """

    parser = argparse.ArgumentParser(
        description='Store a webpage into a single MIME-HTML file',
        usage='getpage [options] url')

    parser.add_argument("url",
                        help="the url of the webpage we should get")
    parser.add_argument(
        "-V", "--version", action='version', version='getpage 0.0.5',
        help="the version of the programm")
    parser.add_argument("-o", "--output", dest="filename",
                        help="name of the outputfile FILE", metavar="FILE")
    parser.add_argument("-d", "--debug",
                        action="store_false", dest="verbose", default=True,
                        help="print debug and verbose messages to stdout")
    parser.add_argument("-v", "--verbose",
                        action="store_false", dest="verbose", default=True,
                        help="print verbose messages to stdout")
    parser.add_argument("-q", "--quiet",
                        action="store_false", dest="verbose", default=True,
                        help="don't print messages to stdout")
    parser.add_argument("-O", "--output-directory",
                        help="name of the outputfile DIRECTORY",
                        metavar="DIRECTORY")

    options = parser.parse_args()
    page = GetPage(options)
    page.retrieve(options.url)
